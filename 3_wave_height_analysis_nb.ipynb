{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Add wave height data to the location and time of each piracy incident"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cce67009833e8fc"
  },
  {
   "cell_type": "markdown",
   "id": "901cd57d",
   "metadata": {},
   "source": [
    "#### This notebook outlines efforts to augment our original piracy event dataframe with the meteorological data  surrounding those events to see if any trends existed between weather factors and acts of piracy. The test dataset came from the Copernicus Marine Data Store. The methods proved effective but due to time constraints only wave heights were augmented for a dataset that covered 3 of the 30 year period. There are other datasets that provide the desired information on the Copernicus site, but future work would be tuning this method to extract the pertinent information from those other datasets. \n",
    "\n",
    "Copernicus Marine Data Store: (https://data.marine.copernicus.eu/products)\n",
    "Ocean Wave Data 2021-2024: https://data.marine.copernicus.eu/product/GLOBAL_ANALYSISFORECAST_WAV_001_027/description\n",
    "\n",
    "NOTE: This notebook demonstrates the process used to arrive at the final product. The clean code that takes the dataset and spits out a csv with the augemented wave heights is in the Github repository in Copernicus_Final.py"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install copernicusmarine\n",
    "# !pip install netCDF4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "316dc9ddb331cf15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import copernicusmarine as copernicus_marine\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23006b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product's filename for GLOBAL_ANALYSISFORECAST_WAV_001_027 wave heights \n",
    "datasetID = 'cmems_mod_glo_wav_anfc_0.083deg_PT3H-i'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9b307",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Super nice because its is a 24 GB dataset but doesn't download to my computer. I can work with it here in the notebook \n",
    "# and save the data I actually want to a different file later. Drawback is could lose all I'm working on if connection to server goes down \n",
    "# only three variables I care about\n",
    "# This data is only from 30 Sep 2021 to 25 Mar 2024 - will need to extend with other or just show as use-case\n",
    "credential_path = Path('Data_Files/.copernicusmarine-credentials')\n",
    "DS = copernicus_marine.open_dataset(dataset_id=datasetID, credentials_file=credential_path)\n",
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99657f",
   "metadata": {},
   "source": [
    "The variable I care about:\n",
    "1. VHM0 [m]\n",
    "    Spectral significant wave height (Hm0)\n",
    "    sea_surface_wave_significant_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get full list of variables available to dataset\n",
    "DS.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of dimensions\n",
    "DS.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27698eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get info on specific variable\n",
    "DS.VHM0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e46fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info on specific dimensions:\n",
    "DS.time, DS.latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in clean dataset\n",
    "piracy_df = pd.read_csv(Path('Data_Files/[Clean] IMO Piracy - 2000 to 2022 (PDV 01-2023).csv'))\n",
    "\n",
    "# Drop lat/long nulls: actually useful info on map\n",
    "piracy_df_map = piracy_df.dropna(subset=['Latitude','Longitude'])\n",
    "\n",
    "# Show result\n",
    "piracy_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert piracy_df_names incident dates to datetimes\n",
    "piracy_df_map.loc[:,'Incident Date'] = pd.to_datetime(piracy_df_map.loc[:,'Incident Date'])\n",
    "piracy_df_map['Incident Date'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b48fe",
   "metadata": {},
   "source": [
    "# Testing process on one Piracy Event:\n",
    "\n",
    "Row entry:\n",
    "\n",
    "5/28/2022\tMagnum Energy\tMarshall Islands\tBulk carrier\tIn international waters\t1.141666667\t103.475\tNot Reported\tStore Rooms\tSteaming\tKnives\tFALSE\tFALSE\tFALSE\tFALSE\tFALSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "piracy_df_names = piracy_df_map.set_index('Ship Name')\n",
    "piracy_df_names.loc['Magnum Energy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64546f",
   "metadata": {},
   "source": [
    "#### Step 1: Determine my buffer / can play with this once I start seeing data or not seeing data\n",
    "\n",
    "\n",
    "#### Step 2: Extract the lat, lon from piracy event\n",
    "\n",
    "    \n",
    "#### Step 3: Create a subset of data with the buffer to the Magnum Energy event "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# First testing the time buffer for the specific instance, then putting it into a loop\n",
    "# Setting buffers so I have data that straddles the event in a 0.1x0.1 degree box lat/lon and 1 day (30 mins before 30 after)\n",
    "# Will play to tune the buffers to get as small a dataset as possible \n",
    "time_buffer = pd.Timedelta(0.5, unit=\"h\") #d \"day\", h \"hour\", m \"minute\"\n",
    "lat_buffer = 0.05 #degree \n",
    "lon_buffer = 0.05 #degree \n",
    "\n",
    "# Step 2\n",
    "# Set the lat and lon to the Magnum Energy event\n",
    "lat = piracy_df_names.loc['Magnum Energy'].Latitude\n",
    "lon = piracy_df_names.loc['Magnum Energy'].Longitude\n",
    "time = piracy_df_names.loc['Magnum Energy']['Incident Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Use the buffer to make a subset of the weather data for points around the event\n",
    "lat_add = lat + lat_buffer\n",
    "lat_subtract = lat - lat_buffer\n",
    "lon_add = lon + lon_buffer\n",
    "lon_subtract = lon - lon_buffer\n",
    "time_add = time + time_buffer\n",
    "time_subtract = time - time_buffer\n",
    "\n",
    "# Create my data subset for the bubble around this specific piracy event\n",
    "subset_Magnum_Energy = DS['VHM0'].sel(\n",
    "    latitude = slice(lat_subtract,lat_add),\n",
    "    longitude = slice(lon_subtract,lon_add),\n",
    "    time = slice(time_subtract, time_add))\n",
    "subset_Magnum_Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccf31d",
   "metadata": {},
   "source": [
    "#### Inspecting the dataset, tuning was perfect (maybe by luck) and I got one reading very close to the event location at that time. \n",
    "\n",
    "#### If tuning is \"imperfect\" and I get more data points \"around\" the event, the values for wave height (my principle variable of interest) are means, and I can further average them to get a rough estimate of the wave height (indicator of sea state) at that time. Ultimately still outputting one value for that event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271dd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lat, lon)\n",
    "# NOT HALF BAD MATEY - not sure if my dimension buffer will always filter out leaving only one but let's keep sailing\n",
    "# Also, of note, these readings are for the day, so a good bit of variability (report didn't have hour/minute just day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subset_Magnum_Energy.to_dataframe()\n",
    "df\n",
    "# Notice there is a NaN value for the max wave height VCMX......don't really need it....or the wave direciton for that matter. \n",
    "# But it raises the question of what do I do if I have a NaN value and have to expand the buffer, thus letting in potentially\n",
    "# more than one value for a particular coordinate? That is when I'd use the nearest method or .minarg stack overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f8c37",
   "metadata": {},
   "source": [
    "# Now build a function that builds these subsets and extracts the wave heights for each piracy event in our piracy dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2642e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For this case with the wave data from 30 Sep 2021 to 25 Mar 2024 \n",
    "DS_start_date = datetime.date(2021,9,30)\n",
    "DS_end_date = datetime.date(2024,3,25)\n",
    "\n",
    "def get_wave_height(row):\n",
    "    if row['Incident Date'] >= DS_start_date:\n",
    "        # print(row['Incident_Date'])\n",
    "        lat = row['Latitude']\n",
    "        lon = row['Longitude']\n",
    "        \n",
    "        # Use the buffer to make a subset of the weather data for points around the event\n",
    "        lat_add = lat + lat_buffer\n",
    "        lat_subtract = lat - lat_buffer\n",
    "        lon_add = lon + lon_buffer\n",
    "        lon_subtract = lon - lon_buffer\n",
    "        time = row['Incident Date']\n",
    "        time_add = time + time_buffer\n",
    "        time_subtract = time - time_buffer\n",
    "        \n",
    "        # Create my data subset for the bubble around this specific piracy event for wave height\n",
    "        # Hopefully this is only going to return one value for each point but it may return more or none\n",
    "        subset = DS[['VHM0', 'VMDR', 'VCMX']].sel(\n",
    "            latitude = slice(lat_subtract,lat_add),\n",
    "            longitude = slice(lon_subtract,lon_add),\n",
    "            time = slice(time_subtract, time_add))\n",
    "        \n",
    "        return subset['VHM0'].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to augment this data to the new matrix \n",
    "piracy_df_map[\"Wave Height\"] = piracy_df_map.apply(get_wave_height, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccba863",
   "metadata": {},
   "outputs": [],
   "source": [
    "piracy_df_map[piracy_df_map[\"Wave Height\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ed2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out to csv file for analysis (145 events updated)\n",
    "piracy_df_map.to_csv(Path('./Results/piracy_df_waves.csv'), index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1e752",
   "metadata": {},
   "source": [
    "# Successful method. Would extend in future work to build out weather data for these piracy events. "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "94a289fc82e87d37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
